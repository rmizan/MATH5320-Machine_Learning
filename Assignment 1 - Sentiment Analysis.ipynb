{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setiment Analysis on Emotion Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -      By Mizanur Rahman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Friday April 17, 2020\n",
    "\n",
    "@author: Mizanur Rahman\n",
    "\n",
    "Source: https://www.kaggle.com/praveengovi/emotions-dataset-for-nlp\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np    # increases efficiency of matrix operations\n",
    "import pandas as pd   # reads in data files of mixed data types\n",
    "import re             # regular expressions to find/replace strings\n",
    "import nltk           # natural language toolkit\n",
    "from nltk.corpus import stopwords   # get list of stopwords to filter\n",
    "                                    # out non-sentiment filler words\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # make the stopword list a set\n",
    "                                             # to increase speed of comparisons\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/mrahman1s/Documents/MML 5320/emotions-dataset-for-nlp/train_data.txt\", header=0, delimiter=\"\\t\", quoting=3)    \n",
    "# read the training data stored in \"trainingDataXXXX.txt\"\n",
    "#test = pd.read_csv(\"testData.txt\", header=0, delimiter=\"\\t\", quoting=3)     \n",
    "# read the test data stored in testData.txt\n",
    "# note: data files are tab delimited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review\n",
      "sentiment\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns: \n",
    "    print(col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                  i didnt feel humiliated\n",
      "1        i can go from feeling so hopeless to so damned...\n",
      "2         im grabbing a minute to post i feel greedy wrong\n",
      "3        i am ever feeling nostalgic about the fireplac...\n",
      "4                                     i am feeling grouchy\n",
      "5        ive been feeling a little burdened lately wasn...\n",
      "6        ive been taking or milligrams or times recomme...\n",
      "7        i feel as confused about life as a teenager or...\n",
      "8        i have been with petronas for years i feel tha...\n",
      "9                                      i feel romantic too\n",
      "10       i feel like i have to make the suffering i m s...\n",
      "11       i do feel that running is a divine experience ...\n",
      "12       i think it s the easiest time of year to feel ...\n",
      "13                      i feel low energy i m just thirsty\n",
      "14       i have immense sympathy with the general point...\n",
      "15         i do not feel reassured anxiety is on each side\n",
      "16                    i didnt really feel that embarrassed\n",
      "17                 i feel pretty pathetic most of the time\n",
      "18       i started feeling sentimental about dolls i ha...\n",
      "19       i now feel compromised and skeptical of the va...\n",
      "20       i feel irritated and rejected without anyone d...\n",
      "21       i am feeling completely overwhelmed i have two...\n",
      "22         i have the feeling she was amused and delighted\n",
      "23       i was able to help chai lifeline with your sup...\n",
      "24       i already feel like i fucked up though because...\n",
      "25       i still love my so and wish the best for him i...\n",
      "26       i feel so inhibited in someone elses kitchen l...\n",
      "27                  i become overwhelmed and feel defeated\n",
      "28       i feel kinda appalled that she feels like she ...\n",
      "29       i feel more superior dead chicken or grieving ...\n",
      "                               ...                        \n",
      "19970    i feel when i am thrilled with my hair i have ...\n",
      "19971    i feel confident that it wasn t my company tha...\n",
      "19972    im feeling a little tender and mashed today an...\n",
      "19973    i found myself feeling a bit shamed defensive ...\n",
      "19974                    i love if i feel a cold coming on\n",
      "19975    i am feeling so helpless ma i am being unable ...\n",
      "19976    i trust heavily when i feel that the trust is ...\n",
      "19977    i anticipated feeling ecstatic jubilant over t...\n",
      "19978    i can feel all supportive and jrock ish in sch...\n",
      "19979    i guess it s all about trying to internalize t...\n",
      "19980    i wrote feel there rather than think or believ...\n",
      "19981    i spent a lot of time feeling overwhelmed with...\n",
      "19982    i feel like the world is just being bitter and...\n",
      "19983    i see people who have accomplished so much mor...\n",
      "19984         i have i feel pathetic for lying if i say no\n",
      "19985    i started to see a concerning pattern i d rush...\n",
      "19986    i had been feeling fabulous and full of energy...\n",
      "19987    i feel im supposed to hate dams amp all the co...\n",
      "19988    i feel like i got to know her a bit and what i...\n",
      "19989    im okay but feeling a little apprehensive as m...\n",
      "19990    i just feel too overwhelmed i can t see the fo...\n",
      "19991    i cant help but feel sentimental about the fac...\n",
      "19992    i feel i should make is how surprised but ente...\n",
      "19993                             i feel so tortured by it\n",
      "19994    i feel a bit rude leaving you hanging there fr...\n",
      "19995    im having ssa examination tomorrow in the morn...\n",
      "19996    i constantly worry about their fight against n...\n",
      "19997    i feel its important to share this info for th...\n",
      "19998    i truly feel that if you are passionate enough...\n",
      "19999    i feel like i just wanna buy any cute make up ...\n",
      "Name: review, Length: 20000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[:,\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating on training data...\n",
      "\n",
      "Cleaning all of the data\n",
      "Cleaning review number 50 out of 16000\n",
      "Cleaning review number 100 out of 16000\n",
      "Cleaning review number 150 out of 16000\n",
      "Cleaning review number 200 out of 16000\n",
      "Cleaning review number 250 out of 16000\n",
      "Cleaning review number 300 out of 16000\n",
      "Cleaning review number 350 out of 16000\n",
      "Cleaning review number 400 out of 16000\n",
      "Cleaning review number 450 out of 16000\n",
      "Cleaning review number 500 out of 16000\n",
      "Cleaning review number 550 out of 16000\n",
      "Cleaning review number 600 out of 16000\n",
      "Cleaning review number 650 out of 16000\n",
      "Cleaning review number 700 out of 16000\n",
      "Cleaning review number 750 out of 16000\n",
      "Cleaning review number 800 out of 16000\n",
      "Cleaning review number 850 out of 16000\n",
      "Cleaning review number 900 out of 16000\n",
      "Cleaning review number 950 out of 16000\n",
      "Cleaning review number 1000 out of 16000\n",
      "Cleaning review number 1050 out of 16000\n",
      "Cleaning review number 1100 out of 16000\n",
      "Cleaning review number 1150 out of 16000\n",
      "Cleaning review number 1200 out of 16000\n",
      "Cleaning review number 1250 out of 16000\n",
      "Cleaning review number 1300 out of 16000\n",
      "Cleaning review number 1350 out of 16000\n",
      "Cleaning review number 1400 out of 16000\n",
      "Cleaning review number 1450 out of 16000\n",
      "Cleaning review number 1500 out of 16000\n",
      "Cleaning review number 1550 out of 16000\n",
      "Cleaning review number 1600 out of 16000\n",
      "Cleaning review number 1650 out of 16000\n",
      "Cleaning review number 1700 out of 16000\n",
      "Cleaning review number 1750 out of 16000\n",
      "Cleaning review number 1800 out of 16000\n",
      "Cleaning review number 1850 out of 16000\n",
      "Cleaning review number 1900 out of 16000\n",
      "Cleaning review number 1950 out of 16000\n",
      "Cleaning review number 2000 out of 16000\n",
      "Cleaning review number 2050 out of 16000\n",
      "Cleaning review number 2100 out of 16000\n",
      "Cleaning review number 2150 out of 16000\n",
      "Cleaning review number 2200 out of 16000\n",
      "Cleaning review number 2250 out of 16000\n",
      "Cleaning review number 2300 out of 16000\n",
      "Cleaning review number 2350 out of 16000\n",
      "Cleaning review number 2400 out of 16000\n",
      "Cleaning review number 2450 out of 16000\n",
      "Cleaning review number 2500 out of 16000\n",
      "Cleaning review number 2550 out of 16000\n",
      "Cleaning review number 2600 out of 16000\n",
      "Cleaning review number 2650 out of 16000\n",
      "Cleaning review number 2700 out of 16000\n",
      "Cleaning review number 2750 out of 16000\n",
      "Cleaning review number 2800 out of 16000\n",
      "Cleaning review number 2850 out of 16000\n",
      "Cleaning review number 2900 out of 16000\n",
      "Cleaning review number 2950 out of 16000\n",
      "Cleaning review number 3000 out of 16000\n",
      "Cleaning review number 3050 out of 16000\n",
      "Cleaning review number 3100 out of 16000\n",
      "Cleaning review number 3150 out of 16000\n",
      "Cleaning review number 3200 out of 16000\n",
      "Cleaning review number 3250 out of 16000\n",
      "Cleaning review number 3300 out of 16000\n",
      "Cleaning review number 3350 out of 16000\n",
      "Cleaning review number 3400 out of 16000\n",
      "Cleaning review number 3450 out of 16000\n",
      "Cleaning review number 3500 out of 16000\n",
      "Cleaning review number 3550 out of 16000\n",
      "Cleaning review number 3600 out of 16000\n",
      "Cleaning review number 3650 out of 16000\n",
      "Cleaning review number 3700 out of 16000\n",
      "Cleaning review number 3750 out of 16000\n",
      "Cleaning review number 3800 out of 16000\n",
      "Cleaning review number 3850 out of 16000\n",
      "Cleaning review number 3900 out of 16000\n",
      "Cleaning review number 3950 out of 16000\n",
      "Cleaning review number 4000 out of 16000\n",
      "Cleaning review number 4050 out of 16000\n",
      "Cleaning review number 4100 out of 16000\n",
      "Cleaning review number 4150 out of 16000\n",
      "Cleaning review number 4200 out of 16000\n",
      "Cleaning review number 4250 out of 16000\n",
      "Cleaning review number 4300 out of 16000\n",
      "Cleaning review number 4350 out of 16000\n",
      "Cleaning review number 4400 out of 16000\n",
      "Cleaning review number 4450 out of 16000\n",
      "Cleaning review number 4500 out of 16000\n",
      "Cleaning review number 4550 out of 16000\n",
      "Cleaning review number 4600 out of 16000\n",
      "Cleaning review number 4650 out of 16000\n",
      "Cleaning review number 4700 out of 16000\n",
      "Cleaning review number 4750 out of 16000\n",
      "Cleaning review number 4800 out of 16000\n",
      "Cleaning review number 4850 out of 16000\n",
      "Cleaning review number 4900 out of 16000\n",
      "Cleaning review number 4950 out of 16000\n",
      "Cleaning review number 5000 out of 16000\n",
      "Cleaning review number 5050 out of 16000\n",
      "Cleaning review number 5100 out of 16000\n",
      "Cleaning review number 5150 out of 16000\n",
      "Cleaning review number 5200 out of 16000\n",
      "Cleaning review number 5250 out of 16000\n",
      "Cleaning review number 5300 out of 16000\n",
      "Cleaning review number 5350 out of 16000\n",
      "Cleaning review number 5400 out of 16000\n",
      "Cleaning review number 5450 out of 16000\n",
      "Cleaning review number 5500 out of 16000\n",
      "Cleaning review number 5550 out of 16000\n",
      "Cleaning review number 5600 out of 16000\n",
      "Cleaning review number 5650 out of 16000\n",
      "Cleaning review number 5700 out of 16000\n",
      "Cleaning review number 5750 out of 16000\n",
      "Cleaning review number 5800 out of 16000\n",
      "Cleaning review number 5850 out of 16000\n",
      "Cleaning review number 5900 out of 16000\n",
      "Cleaning review number 5950 out of 16000\n",
      "Cleaning review number 6000 out of 16000\n",
      "Cleaning review number 6050 out of 16000\n",
      "Cleaning review number 6100 out of 16000\n",
      "Cleaning review number 6150 out of 16000\n",
      "Cleaning review number 6200 out of 16000\n",
      "Cleaning review number 6250 out of 16000\n",
      "Cleaning review number 6300 out of 16000\n",
      "Cleaning review number 6350 out of 16000\n",
      "Cleaning review number 6400 out of 16000\n",
      "Cleaning review number 6450 out of 16000\n",
      "Cleaning review number 6500 out of 16000\n",
      "Cleaning review number 6550 out of 16000\n",
      "Cleaning review number 6600 out of 16000\n",
      "Cleaning review number 6650 out of 16000\n",
      "Cleaning review number 6700 out of 16000\n",
      "Cleaning review number 6750 out of 16000\n",
      "Cleaning review number 6800 out of 16000\n",
      "Cleaning review number 6850 out of 16000\n",
      "Cleaning review number 6900 out of 16000\n",
      "Cleaning review number 6950 out of 16000\n",
      "Cleaning review number 7000 out of 16000\n",
      "Cleaning review number 7050 out of 16000\n",
      "Cleaning review number 7100 out of 16000\n",
      "Cleaning review number 7150 out of 16000\n",
      "Cleaning review number 7200 out of 16000\n",
      "Cleaning review number 7250 out of 16000\n",
      "Cleaning review number 7300 out of 16000\n",
      "Cleaning review number 7350 out of 16000\n",
      "Cleaning review number 7400 out of 16000\n",
      "Cleaning review number 7450 out of 16000\n",
      "Cleaning review number 7500 out of 16000\n",
      "Cleaning review number 7550 out of 16000\n",
      "Cleaning review number 7600 out of 16000\n",
      "Cleaning review number 7650 out of 16000\n",
      "Cleaning review number 7700 out of 16000\n",
      "Cleaning review number 7750 out of 16000\n",
      "Cleaning review number 7800 out of 16000\n",
      "Cleaning review number 7850 out of 16000\n",
      "Cleaning review number 7900 out of 16000\n",
      "Cleaning review number 7950 out of 16000\n",
      "Cleaning review number 8000 out of 16000\n",
      "Cleaning review number 8050 out of 16000\n",
      "Cleaning review number 8100 out of 16000\n",
      "Cleaning review number 8150 out of 16000\n",
      "Cleaning review number 8200 out of 16000\n",
      "Cleaning review number 8250 out of 16000\n",
      "Cleaning review number 8300 out of 16000\n",
      "Cleaning review number 8350 out of 16000\n",
      "Cleaning review number 8400 out of 16000\n",
      "Cleaning review number 8450 out of 16000\n",
      "Cleaning review number 8500 out of 16000\n",
      "Cleaning review number 8550 out of 16000\n",
      "Cleaning review number 8600 out of 16000\n",
      "Cleaning review number 8650 out of 16000\n",
      "Cleaning review number 8700 out of 16000\n",
      "Cleaning review number 8750 out of 16000\n",
      "Cleaning review number 8800 out of 16000\n",
      "Cleaning review number 8850 out of 16000\n",
      "Cleaning review number 8900 out of 16000\n",
      "Cleaning review number 8950 out of 16000\n",
      "Cleaning review number 9000 out of 16000\n",
      "Cleaning review number 9050 out of 16000\n",
      "Cleaning review number 9100 out of 16000\n",
      "Cleaning review number 9150 out of 16000\n",
      "Cleaning review number 9200 out of 16000\n",
      "Cleaning review number 9250 out of 16000\n",
      "Cleaning review number 9300 out of 16000\n",
      "Cleaning review number 9350 out of 16000\n",
      "Cleaning review number 9400 out of 16000\n",
      "Cleaning review number 9450 out of 16000\n",
      "Cleaning review number 9500 out of 16000\n",
      "Cleaning review number 9550 out of 16000\n",
      "Cleaning review number 9600 out of 16000\n",
      "Cleaning review number 9650 out of 16000\n",
      "Cleaning review number 9700 out of 16000\n",
      "Cleaning review number 9750 out of 16000\n",
      "Cleaning review number 9800 out of 16000\n",
      "Cleaning review number 9850 out of 16000\n",
      "Cleaning review number 9900 out of 16000\n",
      "Cleaning review number 9950 out of 16000\n",
      "Cleaning review number 10000 out of 16000\n",
      "Cleaning review number 10050 out of 16000\n",
      "Cleaning review number 10100 out of 16000\n",
      "Cleaning review number 10150 out of 16000\n",
      "Cleaning review number 10200 out of 16000\n",
      "Cleaning review number 10250 out of 16000\n",
      "Cleaning review number 10300 out of 16000\n",
      "Cleaning review number 10350 out of 16000\n",
      "Cleaning review number 10400 out of 16000\n",
      "Cleaning review number 10450 out of 16000\n",
      "Cleaning review number 10500 out of 16000\n",
      "Cleaning review number 10550 out of 16000\n",
      "Cleaning review number 10600 out of 16000\n",
      "Cleaning review number 10650 out of 16000\n",
      "Cleaning review number 10700 out of 16000\n",
      "Cleaning review number 10750 out of 16000\n",
      "Cleaning review number 10800 out of 16000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning review number 10850 out of 16000\n",
      "Cleaning review number 10900 out of 16000\n",
      "Cleaning review number 10950 out of 16000\n",
      "Cleaning review number 11000 out of 16000\n",
      "Cleaning review number 11050 out of 16000\n",
      "Cleaning review number 11100 out of 16000\n",
      "Cleaning review number 11150 out of 16000\n",
      "Cleaning review number 11200 out of 16000\n",
      "Cleaning review number 11250 out of 16000\n",
      "Cleaning review number 11300 out of 16000\n",
      "Cleaning review number 11350 out of 16000\n",
      "Cleaning review number 11400 out of 16000\n",
      "Cleaning review number 11450 out of 16000\n",
      "Cleaning review number 11500 out of 16000\n",
      "Cleaning review number 11550 out of 16000\n",
      "Cleaning review number 11600 out of 16000\n",
      "Cleaning review number 11650 out of 16000\n",
      "Cleaning review number 11700 out of 16000\n",
      "Cleaning review number 11750 out of 16000\n",
      "Cleaning review number 11800 out of 16000\n",
      "Cleaning review number 11850 out of 16000\n",
      "Cleaning review number 11900 out of 16000\n",
      "Cleaning review number 11950 out of 16000\n",
      "Cleaning review number 12000 out of 16000\n",
      "Cleaning review number 12050 out of 16000\n",
      "Cleaning review number 12100 out of 16000\n",
      "Cleaning review number 12150 out of 16000\n",
      "Cleaning review number 12200 out of 16000\n",
      "Cleaning review number 12250 out of 16000\n",
      "Cleaning review number 12300 out of 16000\n",
      "Cleaning review number 12350 out of 16000\n",
      "Cleaning review number 12400 out of 16000\n",
      "Cleaning review number 12450 out of 16000\n",
      "Cleaning review number 12500 out of 16000\n",
      "Cleaning review number 12550 out of 16000\n",
      "Cleaning review number 12600 out of 16000\n",
      "Cleaning review number 12650 out of 16000\n",
      "Cleaning review number 12700 out of 16000\n",
      "Cleaning review number 12750 out of 16000\n",
      "Cleaning review number 12800 out of 16000\n",
      "Cleaning review number 12850 out of 16000\n",
      "Cleaning review number 12900 out of 16000\n",
      "Cleaning review number 12950 out of 16000\n",
      "Cleaning review number 13000 out of 16000\n",
      "Cleaning review number 13050 out of 16000\n",
      "Cleaning review number 13100 out of 16000\n",
      "Cleaning review number 13150 out of 16000\n",
      "Cleaning review number 13200 out of 16000\n",
      "Cleaning review number 13250 out of 16000\n",
      "Cleaning review number 13300 out of 16000\n",
      "Cleaning review number 13350 out of 16000\n",
      "Cleaning review number 13400 out of 16000\n",
      "Cleaning review number 13450 out of 16000\n",
      "Cleaning review number 13500 out of 16000\n",
      "Cleaning review number 13550 out of 16000\n",
      "Cleaning review number 13600 out of 16000\n",
      "Cleaning review number 13650 out of 16000\n",
      "Cleaning review number 13700 out of 16000\n",
      "Cleaning review number 13750 out of 16000\n",
      "Cleaning review number 13800 out of 16000\n",
      "Cleaning review number 13850 out of 16000\n",
      "Cleaning review number 13900 out of 16000\n",
      "Cleaning review number 13950 out of 16000\n",
      "Cleaning review number 14000 out of 16000\n",
      "Cleaning review number 14050 out of 16000\n",
      "Cleaning review number 14100 out of 16000\n",
      "Cleaning review number 14150 out of 16000\n",
      "Cleaning review number 14200 out of 16000\n",
      "Cleaning review number 14250 out of 16000\n",
      "Cleaning review number 14300 out of 16000\n",
      "Cleaning review number 14350 out of 16000\n",
      "Cleaning review number 14400 out of 16000\n",
      "Cleaning review number 14450 out of 16000\n",
      "Cleaning review number 14500 out of 16000\n",
      "Cleaning review number 14550 out of 16000\n",
      "Cleaning review number 14600 out of 16000\n",
      "Cleaning review number 14650 out of 16000\n",
      "Cleaning review number 14700 out of 16000\n",
      "Cleaning review number 14750 out of 16000\n",
      "Cleaning review number 14800 out of 16000\n",
      "Cleaning review number 14850 out of 16000\n",
      "Cleaning review number 14900 out of 16000\n",
      "Cleaning review number 14950 out of 16000\n",
      "Cleaning review number 15000 out of 16000\n",
      "Cleaning review number 15050 out of 16000\n",
      "Cleaning review number 15100 out of 16000\n",
      "Cleaning review number 15150 out of 16000\n",
      "Cleaning review number 15200 out of 16000\n",
      "Cleaning review number 15250 out of 16000\n",
      "Cleaning review number 15300 out of 16000\n",
      "Cleaning review number 15350 out of 16000\n",
      "Cleaning review number 15400 out of 16000\n",
      "Cleaning review number 15450 out of 16000\n",
      "Cleaning review number 15500 out of 16000\n",
      "Cleaning review number 15550 out of 16000\n",
      "Cleaning review number 15600 out of 16000\n",
      "Cleaning review number 15650 out of 16000\n",
      "Cleaning review number 15700 out of 16000\n",
      "Cleaning review number 15750 out of 16000\n",
      "Cleaning review number 15800 out of 16000\n",
      "Cleaning review number 15850 out of 16000\n",
      "Cleaning review number 15900 out of 16000\n",
      "Cleaning review number 15950 out of 16000\n",
      "Cleaning review number 16000 out of 16000\n",
      "Finished cleaning all of the data\n",
      "\n",
      "Generating bag of words...\n",
      "Training the logistic regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrahman1s\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training the model\n",
      "\n",
      "Operating on test data...\n",
      "\n",
      "Cleaning all of the data\n",
      "Cleaning review number 50 out of 4000\n",
      "Cleaning review number 100 out of 4000\n",
      "Cleaning review number 150 out of 4000\n",
      "Cleaning review number 200 out of 4000\n",
      "Cleaning review number 250 out of 4000\n",
      "Cleaning review number 300 out of 4000\n",
      "Cleaning review number 350 out of 4000\n",
      "Cleaning review number 400 out of 4000\n",
      "Cleaning review number 450 out of 4000\n",
      "Cleaning review number 500 out of 4000\n",
      "Cleaning review number 550 out of 4000\n",
      "Cleaning review number 600 out of 4000\n",
      "Cleaning review number 650 out of 4000\n",
      "Cleaning review number 700 out of 4000\n",
      "Cleaning review number 750 out of 4000\n",
      "Cleaning review number 800 out of 4000\n",
      "Cleaning review number 850 out of 4000\n",
      "Cleaning review number 900 out of 4000\n",
      "Cleaning review number 950 out of 4000\n",
      "Cleaning review number 1000 out of 4000\n",
      "Cleaning review number 1050 out of 4000\n",
      "Cleaning review number 1100 out of 4000\n",
      "Cleaning review number 1150 out of 4000\n",
      "Cleaning review number 1200 out of 4000\n",
      "Cleaning review number 1250 out of 4000\n",
      "Cleaning review number 1300 out of 4000\n",
      "Cleaning review number 1350 out of 4000\n",
      "Cleaning review number 1400 out of 4000\n",
      "Cleaning review number 1450 out of 4000\n",
      "Cleaning review number 1500 out of 4000\n",
      "Cleaning review number 1550 out of 4000\n",
      "Cleaning review number 1600 out of 4000\n",
      "Cleaning review number 1650 out of 4000\n",
      "Cleaning review number 1700 out of 4000\n",
      "Cleaning review number 1750 out of 4000\n",
      "Cleaning review number 1800 out of 4000\n",
      "Cleaning review number 1850 out of 4000\n",
      "Cleaning review number 1900 out of 4000\n",
      "Cleaning review number 1950 out of 4000\n",
      "Cleaning review number 2000 out of 4000\n",
      "Cleaning review number 2050 out of 4000\n",
      "Cleaning review number 2100 out of 4000\n",
      "Cleaning review number 2150 out of 4000\n",
      "Cleaning review number 2200 out of 4000\n",
      "Cleaning review number 2250 out of 4000\n",
      "Cleaning review number 2300 out of 4000\n",
      "Cleaning review number 2350 out of 4000\n",
      "Cleaning review number 2400 out of 4000\n",
      "Cleaning review number 2450 out of 4000\n",
      "Cleaning review number 2500 out of 4000\n",
      "Cleaning review number 2550 out of 4000\n",
      "Cleaning review number 2600 out of 4000\n",
      "Cleaning review number 2650 out of 4000\n",
      "Cleaning review number 2700 out of 4000\n",
      "Cleaning review number 2750 out of 4000\n",
      "Cleaning review number 2800 out of 4000\n",
      "Cleaning review number 2850 out of 4000\n",
      "Cleaning review number 2900 out of 4000\n",
      "Cleaning review number 2950 out of 4000\n",
      "Cleaning review number 3000 out of 4000\n",
      "Cleaning review number 3050 out of 4000\n",
      "Cleaning review number 3100 out of 4000\n",
      "Cleaning review number 3150 out of 4000\n",
      "Cleaning review number 3200 out of 4000\n",
      "Cleaning review number 3250 out of 4000\n",
      "Cleaning review number 3300 out of 4000\n",
      "Cleaning review number 3350 out of 4000\n",
      "Cleaning review number 3400 out of 4000\n",
      "Cleaning review number 3450 out of 4000\n",
      "Cleaning review number 3500 out of 4000\n",
      "Cleaning review number 3550 out of 4000\n",
      "Cleaning review number 3600 out of 4000\n",
      "Cleaning review number 3650 out of 4000\n",
      "Cleaning review number 3700 out of 4000\n",
      "Cleaning review number 3750 out of 4000\n",
      "Cleaning review number 3800 out of 4000\n",
      "Cleaning review number 3850 out of 4000\n",
      "Cleaning review number 3900 out of 4000\n",
      "Cleaning review number 3950 out of 4000\n",
      "Cleaning review number 4000 out of 4000\n",
      "Finished cleaning all of the data\n",
      "\n",
      "The accuracy of the model in predicting emotions dataset sentiment is 96%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"review\"], df[\"sentiment\"], test_size=0.2)\n",
    "\n",
    "   \n",
    "\"\"\" clean_my_text(): cleans the data with several replacements/deletions,\n",
    "    tokenizes the text, and removes stopwords\n",
    "    input: string data\n",
    "    output: cleaned string data ready for sentiment analysis\n",
    "\"\"\"\n",
    "def clean_my_text(text):\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)      # quick removal of HTML tags\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)  # strip out all non-alpha chars\n",
    "    text = text.strip().lower()            # convert all text to lowercase\n",
    "    text = re.sub(\" s \", \" \", text)        # remove isolated s chars that \n",
    "                                           # result from cleaning possessives\n",
    "\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()  # tokenizes text using\n",
    "                                                       # smart divisions\n",
    "    tokens = tokenizer.tokenize(text)      # store results in tokens\n",
    "    \n",
    "\n",
    "    unstopped = []                         # holds the cleaned data string\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:         # removes stopwords\n",
    "            unstopped.append(word)         # adds word to unstopped string\n",
    "    stemmer = nltk.stem.WordNetLemmatizer()   # consolidates different\n",
    "                                                # word forms\n",
    "    cleanText = \" \".join(stemmer.lemmatize(token) for token in unstopped)\n",
    "                # joins final clean tokens into a string\n",
    "    return cleanText\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" clean_my_data() calls clean_my_text for each line of text in a dataset\n",
    "    category  \n",
    "    input: data file containing raw text  \n",
    "    output: data file containing cleaned text entries\n",
    "\"\"\"\n",
    "def clean_my_data(dataList):\n",
    "    print(\"Cleaning all of the data\")\n",
    "    i = 0\n",
    "    for textEntry in dataList:              # reads line of text under \n",
    "                                                    # review category\n",
    "        cleanElement = clean_my_text(textEntry)     # cleans line of text\n",
    "        dataList[i] = cleanElement   # stores cleaned text\n",
    "        i = i + 1\n",
    "        if (i%50 == 0):\n",
    "            print(\"Cleaning review number\", i, \"out of\", len(dataList))\n",
    "    print(\"Finished cleaning all of the data\\n\")\n",
    "    return dataList\n",
    "\n",
    "\n",
    "print(\"Operating on training data...\\n\")\n",
    "reviews = X_train.tolist()\n",
    "cleanReviewData = clean_my_data(reviews)            # cleans the training data\n",
    "\"\"\" create_bag_of_words() generates the bag of words used to evaluate sentiment\n",
    "    input: cleaned dataset\n",
    "    output: tf-idf weighted sparse matrix\n",
    "\"\"\"\n",
    "def create_bag_of_words(X):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "        # use scikit-learn for vectorization\n",
    "    \n",
    "    print ('Generating bag of words...')\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 ngram_range = (1,2), \\\n",
    "                                 max_features = 10000)\n",
    "        # generates vectorization for ngrams of up to 2 words in length\n",
    "        # this will greatly increase feature size, but gives more accurate\n",
    "        # sentiment analysis since some word combinations have large\n",
    "        # impact on sentiment ie: (\"not good\", \"very fast\")\n",
    "                                                         \n",
    "    train_data_features = vectorizer.fit_transform(X)\n",
    "        # vectorizes sparse matrix\n",
    "    train_data_features = train_data_features.toarray()\n",
    "        # convert to a NumPy array for efficient matrix operations\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf_features = tfidf.fit_transform(train_data_features)\n",
    "        # use tf-idf to weight features - places highest sentiment value on\n",
    "        # low-frequency ngrams that are not too uncommon \n",
    "    return vectorizer, tfidf_features, tfidf\n",
    "\n",
    "\n",
    "\n",
    "vectorizer, tfidf_features, tfidf  = (create_bag_of_words(cleanReviewData))   \n",
    "        # stores the sparse matrix of the tf-idf weighted features\n",
    "\n",
    "\n",
    "\"\"\" train_logistic_regression() uses logistic regression model to\n",
    "    evaluate sentiment\n",
    "    options: C sets how strong regularization will be: large C = small amount\n",
    "    input: tf-idf matrix and the sentiment attached to the training example\n",
    "    output: the trained logistic regression model\n",
    "\"\"\"\n",
    "def train_logistic_regression(features, label):\n",
    "    print (\"Training the logistic regression model...\")\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    ml_model = LogisticRegression(C = 100, random_state = 0, solver = 'liblinear')\n",
    "    ml_model.fit(features, label)\n",
    "    print ('Finished training the model\\n')\n",
    "    return ml_model\n",
    "\n",
    "\n",
    "ml_model = train_logistic_regression(tfidf_features, y_train)\n",
    "    # holds the trained model\n",
    "    \n",
    "print(\"Operating on test data...\\n\")\n",
    "sentiments = X_test.tolist()\n",
    "cleanTestData = clean_my_data(sentiments)\n",
    "    # cleans the test data for accuracy evaluation\n",
    "\n",
    "test_data_features = vectorizer.transform(cleanTestData)\n",
    "test_data_features = test_data_features.toarray()\n",
    "    # vectorizes the test data\n",
    "\n",
    "test_data_tfidf_features = tfidf.fit_transform(test_data_features)\n",
    "test_data_tfidf_features = test_data_tfidf_features.toarray()\n",
    "    # tf-idf of test data ngrams\n",
    "\n",
    "predicted_y = ml_model.predict(test_data_tfidf_features)\n",
    "    # uses the trained logistic regression model to assign sentiment to each\n",
    "    # test data example\n",
    "\n",
    "correctly_identified_y = predicted_y == y_test\n",
    "accuracy = np.mean(correctly_identified_y) * 100\n",
    "print ('The accuracy of the model in predicting emotions dataset sentiment is %.0f%%' %accuracy)\n",
    "    # compares the predicted sentiment (predicted_y) vs the actual \n",
    "# value stored in \"sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
