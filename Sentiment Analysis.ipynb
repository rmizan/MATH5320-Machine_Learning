{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - IMDB Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Feb 11 2019\n",
    "\n",
    "@author: Daniel Mayo\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np    # increases efficiency of matrix operations\n",
    "import pandas as pd   # reads in data files of mixed data types\n",
    "import re             # regular expressions to find/replace strings\n",
    "import nltk           # natural language toolkit\n",
    "from nltk.corpus import stopwords   # get list of stopwords to filter\n",
    "                                    # out non-sentiment filler words\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # make the stopword list a set\n",
    "                                             # to increase speed of comparisons\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/mrahman1s/Documents/MML 5320/Sentiment Analysis with Random Split/trainingData5000.txt\", header=0, delimiter=\"\\t\", quoting=3)    \n",
    "# read the training data stored in \"trainingDataXXXX.txt\"\n",
    "#test = pd.read_csv(\"testData.txt\", header=0, delimiter=\"\\t\", quoting=3)     \n",
    "# read the test data stored in testData.txt\n",
    "# note: data files are tab delimited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\The Classic War of the Worlds\\\"\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8196_8</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I dont know why people think this is such a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7166_2</td>\n",
       "      <td>0</td>\n",
       "      <td>\"This movie could have been very good, but com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10633_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I watched this video at a friend's house. I'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>319_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"A friend of mine bought this film for £1, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8713_10</td>\n",
       "      <td>1</td>\n",
       "      <td>\"&lt;br /&gt;&lt;br /&gt;This movie is full of references....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2486_3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"What happens when an army of wetbacks, towelh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6811_10</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Although I generally do not like remakes beli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11744_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\Mr. Harvey Lights a Candle\\\"\" is anchored by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7369_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I had a feeling that after \\Submerged\\\"\", thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12081_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"note to George Litman, and others: the Myster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3561_4</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Stephen King adaptation (scripted by King him...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4489_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"`The Matrix' was an exciting summer blockbust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3951_2</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Ulli Lommel's 1980 film 'The Boogey Man' is n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3304_10</td>\n",
       "      <td>1</td>\n",
       "      <td>\"This movie is one among the very few Indian m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9352_10</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Most people, especially young people, may not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3374_7</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\Soylent Green\\\"\" is one of the best and most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10782_7</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Michael Stearns plays Mike, a sexually frustr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5414_10</td>\n",
       "      <td>1</td>\n",
       "      <td>\"This happy-go-luck 1939 military swashbuckler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10492_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I would love to have that two hours of my lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3350_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The script for this movie was probably found i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6581_7</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Looking for Quo Vadis at my local video store...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2203_3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Note to all mad scientists everywhere: if you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>689_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"What the ........... is this ? This must, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9152_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Intrigued by the synopsis (every gay video th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6077_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Would anyone really watch this RUBBISH if it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>2906_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"-might contain spoilers... but believe me, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>2634_7</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Lion King 1 1/2 is a very fun and addictive s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>5888_4</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I saw this recently on a faded old VHS tape, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>10652_2</td>\n",
       "      <td>0</td>\n",
       "      <td>\"You know Jason, you know Freddy, and you know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>10411_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"An utterly beautiful film, one of a handful o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>12395_4</td>\n",
       "      <td>0</td>\n",
       "      <td>\"When i got this movie free from my job, along...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4976</th>\n",
       "      <td>4692_10</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Although critically maligned, Johnny Dangerou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>11789_10</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Hello, this little film is interesting especi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4978</th>\n",
       "      <td>8732_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Lately I have been watching a lot of Tom Hank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>11378_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The dudes at MST3K should see this dog of a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>8054_8</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\Enter the Fat Dragon\\\"\" is one of the funnie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>7791_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I've seen the original non-dubbed German vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>8918_3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"...thankfully he hasn't, yet! This is crude, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>111_4</td>\n",
       "      <td>0</td>\n",
       "      <td>\"My interest in Dorothy Stratten caused me to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>10765_10</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Yowsa! If you REALLY want some ACTION, check ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>10758_8</td>\n",
       "      <td>1</td>\n",
       "      <td>\"9/11 is a classic example of cinema verite, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>5995_10</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Not sure if it was right or wrong, but I read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4987</th>\n",
       "      <td>10433_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"American boy Jesse took the train to Vienna i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988</th>\n",
       "      <td>2957_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"What a mess!! Why was this movie made? This, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>10134_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Steven Seagal movies have never been Oscar ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>1499_7</td>\n",
       "      <td>1</td>\n",
       "      <td>\"For fans of 1970s Hammer type horror films, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4991</th>\n",
       "      <td>5203_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I have seen poor movies in my time, but this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>5648_2</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Boston legal has turned its tail and is heade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>4679_10</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Terrific film with a slightly slow start - gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>9007_1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"This movie is some of the worst crap I have e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>3720_2</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Is this film a joke? Is it a comedy? Surely i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4229_10</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Don't waste time reading my review. Go out an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>8042_3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The secret is...this movie blows. Sorry, but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>9669_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"After reading the original play I thought it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>11216_7</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The story is extremely unique.It's about thes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  sentiment                                             review\n",
       "0       5814_8          1  \"With all this stuff going down at the moment ...\n",
       "1       2381_9          1  \"\\The Classic War of the Worlds\\\"\" by Timothy ...\n",
       "2       7759_3          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3       3630_4          0  \"It must be assumed that those who praised thi...\n",
       "4       9495_8          1  \"Superbly trashy and wondrously unpretentious ...\n",
       "5       8196_8          1  \"I dont know why people think this is such a b...\n",
       "6       7166_2          0  \"This movie could have been very good, but com...\n",
       "7      10633_1          0  \"I watched this video at a friend's house. I'm...\n",
       "8        319_1          0  \"A friend of mine bought this film for £1, and...\n",
       "9      8713_10          1  \"<br /><br />This movie is full of references....\n",
       "10      2486_3          0  \"What happens when an army of wetbacks, towelh...\n",
       "11     6811_10          1  \"Although I generally do not like remakes beli...\n",
       "12     11744_9          1  \"\\Mr. Harvey Lights a Candle\\\"\" is anchored by...\n",
       "13      7369_1          0  \"I had a feeling that after \\Submerged\\\"\", thi...\n",
       "14     12081_1          0  \"note to George Litman, and others: the Myster...\n",
       "15      3561_4          0  \"Stephen King adaptation (scripted by King him...\n",
       "16      4489_1          0  \"`The Matrix' was an exciting summer blockbust...\n",
       "17      3951_2          0  \"Ulli Lommel's 1980 film 'The Boogey Man' is n...\n",
       "18     3304_10          1  \"This movie is one among the very few Indian m...\n",
       "19     9352_10          1  \"Most people, especially young people, may not...\n",
       "20      3374_7          1  \"\\Soylent Green\\\"\" is one of the best and most...\n",
       "21     10782_7          1  \"Michael Stearns plays Mike, a sexually frustr...\n",
       "22     5414_10          1  \"This happy-go-luck 1939 military swashbuckler...\n",
       "23     10492_1          0  \"I would love to have that two hours of my lif...\n",
       "24      3350_3          0  The script for this movie was probably found i...\n",
       "25      6581_7          1  \"Looking for Quo Vadis at my local video store...\n",
       "26      2203_3          0  \"Note to all mad scientists everywhere: if you...\n",
       "27       689_1          0  \"What the ........... is this ? This must, wit...\n",
       "28      9152_1          0  \"Intrigued by the synopsis (every gay video th...\n",
       "29      6077_1          0  \"Would anyone really watch this RUBBISH if it ...\n",
       "...        ...        ...                                                ...\n",
       "4970    2906_1          0  \"-might contain spoilers... but believe me, th...\n",
       "4971    2634_7          1  \"Lion King 1 1/2 is a very fun and addictive s...\n",
       "4972    5888_4          0  \"I saw this recently on a faded old VHS tape, ...\n",
       "4973   10652_2          0  \"You know Jason, you know Freddy, and you know...\n",
       "4974   10411_9          1  \"An utterly beautiful film, one of a handful o...\n",
       "4975   12395_4          0  \"When i got this movie free from my job, along...\n",
       "4976   4692_10          1  \"Although critically maligned, Johnny Dangerou...\n",
       "4977  11789_10          1  \"Hello, this little film is interesting especi...\n",
       "4978    8732_9          1  \"Lately I have been watching a lot of Tom Hank...\n",
       "4979   11378_1          0  \"The dudes at MST3K should see this dog of a f...\n",
       "4980    8054_8          1  \"\\Enter the Fat Dragon\\\"\" is one of the funnie...\n",
       "4981    7791_1          0  \"I've seen the original non-dubbed German vers...\n",
       "4982    8918_3          0  \"...thankfully he hasn't, yet! This is crude, ...\n",
       "4983     111_4          0  \"My interest in Dorothy Stratten caused me to ...\n",
       "4984  10765_10          1  \"Yowsa! If you REALLY want some ACTION, check ...\n",
       "4985   10758_8          1  \"9/11 is a classic example of cinema verite, a...\n",
       "4986   5995_10          1  \"Not sure if it was right or wrong, but I read...\n",
       "4987   10433_9          1  \"American boy Jesse took the train to Vienna i...\n",
       "4988    2957_1          0  \"What a mess!! Why was this movie made? This, ...\n",
       "4989   10134_1          0  \"Steven Seagal movies have never been Oscar ma...\n",
       "4990    1499_7          1  \"For fans of 1970s Hammer type horror films, t...\n",
       "4991    5203_1          0  \"I have seen poor movies in my time, but this ...\n",
       "4992    5648_2          0  \"Boston legal has turned its tail and is heade...\n",
       "4993   4679_10          1  \"Terrific film with a slightly slow start - gi...\n",
       "4994    9007_1          0  \"This movie is some of the worst crap I have e...\n",
       "4995    3720_2          0  \"Is this film a joke? Is it a comedy? Surely i...\n",
       "4996   4229_10          1  \"Don't waste time reading my review. Go out an...\n",
       "4997    8042_3          0  \"The secret is...this movie blows. Sorry, but ...\n",
       "4998    9669_9          1  \"After reading the original play I thought it ...\n",
       "4999   11216_7          1  \"The story is extremely unique.It's about thes...\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating on training data...\n",
      "\n",
      "Cleaning all of the data\n",
      "Cleaning review number 50 out of 4000\n",
      "Cleaning review number 100 out of 4000\n",
      "Cleaning review number 150 out of 4000\n",
      "Cleaning review number 200 out of 4000\n",
      "Cleaning review number 250 out of 4000\n",
      "Cleaning review number 300 out of 4000\n",
      "Cleaning review number 350 out of 4000\n",
      "Cleaning review number 400 out of 4000\n",
      "Cleaning review number 450 out of 4000\n",
      "Cleaning review number 500 out of 4000\n",
      "Cleaning review number 550 out of 4000\n",
      "Cleaning review number 600 out of 4000\n",
      "Cleaning review number 650 out of 4000\n",
      "Cleaning review number 700 out of 4000\n",
      "Cleaning review number 750 out of 4000\n",
      "Cleaning review number 800 out of 4000\n",
      "Cleaning review number 850 out of 4000\n",
      "Cleaning review number 900 out of 4000\n",
      "Cleaning review number 950 out of 4000\n",
      "Cleaning review number 1000 out of 4000\n",
      "Cleaning review number 1050 out of 4000\n",
      "Cleaning review number 1100 out of 4000\n",
      "Cleaning review number 1150 out of 4000\n",
      "Cleaning review number 1200 out of 4000\n",
      "Cleaning review number 1250 out of 4000\n",
      "Cleaning review number 1300 out of 4000\n",
      "Cleaning review number 1350 out of 4000\n",
      "Cleaning review number 1400 out of 4000\n",
      "Cleaning review number 1450 out of 4000\n",
      "Cleaning review number 1500 out of 4000\n",
      "Cleaning review number 1550 out of 4000\n",
      "Cleaning review number 1600 out of 4000\n",
      "Cleaning review number 1650 out of 4000\n",
      "Cleaning review number 1700 out of 4000\n",
      "Cleaning review number 1750 out of 4000\n",
      "Cleaning review number 1800 out of 4000\n",
      "Cleaning review number 1850 out of 4000\n",
      "Cleaning review number 1900 out of 4000\n",
      "Cleaning review number 1950 out of 4000\n",
      "Cleaning review number 2000 out of 4000\n",
      "Cleaning review number 2050 out of 4000\n",
      "Cleaning review number 2100 out of 4000\n",
      "Cleaning review number 2150 out of 4000\n",
      "Cleaning review number 2200 out of 4000\n",
      "Cleaning review number 2250 out of 4000\n",
      "Cleaning review number 2300 out of 4000\n",
      "Cleaning review number 2350 out of 4000\n",
      "Cleaning review number 2400 out of 4000\n",
      "Cleaning review number 2450 out of 4000\n",
      "Cleaning review number 2500 out of 4000\n",
      "Cleaning review number 2550 out of 4000\n",
      "Cleaning review number 2600 out of 4000\n",
      "Cleaning review number 2650 out of 4000\n",
      "Cleaning review number 2700 out of 4000\n",
      "Cleaning review number 2750 out of 4000\n",
      "Cleaning review number 2800 out of 4000\n",
      "Cleaning review number 2850 out of 4000\n",
      "Cleaning review number 2900 out of 4000\n",
      "Cleaning review number 2950 out of 4000\n",
      "Cleaning review number 3000 out of 4000\n",
      "Cleaning review number 3050 out of 4000\n",
      "Cleaning review number 3100 out of 4000\n",
      "Cleaning review number 3150 out of 4000\n",
      "Cleaning review number 3200 out of 4000\n",
      "Cleaning review number 3250 out of 4000\n",
      "Cleaning review number 3300 out of 4000\n",
      "Cleaning review number 3350 out of 4000\n",
      "Cleaning review number 3400 out of 4000\n",
      "Cleaning review number 3450 out of 4000\n",
      "Cleaning review number 3500 out of 4000\n",
      "Cleaning review number 3550 out of 4000\n",
      "Cleaning review number 3600 out of 4000\n",
      "Cleaning review number 3650 out of 4000\n",
      "Cleaning review number 3700 out of 4000\n",
      "Cleaning review number 3750 out of 4000\n",
      "Cleaning review number 3800 out of 4000\n",
      "Cleaning review number 3850 out of 4000\n",
      "Cleaning review number 3900 out of 4000\n",
      "Cleaning review number 3950 out of 4000\n",
      "Cleaning review number 4000 out of 4000\n",
      "Finished cleaning all of the data\n",
      "\n",
      "Generating bag of words...\n",
      "Training the logistic regression model...\n",
      "Finished training the model\n",
      "\n",
      "Operating on test data...\n",
      "\n",
      "Cleaning all of the data\n",
      "Cleaning review number 50 out of 1000\n",
      "Cleaning review number 100 out of 1000\n",
      "Cleaning review number 150 out of 1000\n",
      "Cleaning review number 200 out of 1000\n",
      "Cleaning review number 250 out of 1000\n",
      "Cleaning review number 300 out of 1000\n",
      "Cleaning review number 350 out of 1000\n",
      "Cleaning review number 400 out of 1000\n",
      "Cleaning review number 450 out of 1000\n",
      "Cleaning review number 500 out of 1000\n",
      "Cleaning review number 550 out of 1000\n",
      "Cleaning review number 600 out of 1000\n",
      "Cleaning review number 650 out of 1000\n",
      "Cleaning review number 700 out of 1000\n",
      "Cleaning review number 750 out of 1000\n",
      "Cleaning review number 800 out of 1000\n",
      "Cleaning review number 850 out of 1000\n",
      "Cleaning review number 900 out of 1000\n",
      "Cleaning review number 950 out of 1000\n",
      "Cleaning review number 1000 out of 1000\n",
      "Finished cleaning all of the data\n",
      "\n",
      "The accuracy of the model in predicting movie review sentiment is 88%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"review\"], df[\"sentiment\"], test_size=0.2)\n",
    "\n",
    "   \n",
    "\"\"\" clean_my_text(): cleans the data with several replacements/deletions,\n",
    "    tokenizes the text, and removes stopwords\n",
    "    input: string data\n",
    "    output: cleaned string data ready for sentiment analysis\n",
    "\"\"\"\n",
    "def clean_my_text(text):\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)      # quick removal of HTML tags\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)  # strip out all non-alpha chars\n",
    "    text = text.strip().lower()            # convert all text to lowercase\n",
    "    text = re.sub(\" s \", \" \", text)        # remove isolated s chars that \n",
    "                                           # result from cleaning possessives\n",
    "\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()  # tokenizes text using\n",
    "                                                       # smart divisions\n",
    "    tokens = tokenizer.tokenize(text)      # store results in tokens\n",
    "    \n",
    "\n",
    "    unstopped = []                         # holds the cleaned data string\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:         # removes stopwords\n",
    "            unstopped.append(word)         # adds word to unstopped string\n",
    "    stemmer = nltk.stem.WordNetLemmatizer()   # consolidates different\n",
    "                                                # word forms\n",
    "    cleanText = \" \".join(stemmer.lemmatize(token) for token in unstopped)\n",
    "                # joins final clean tokens into a string\n",
    "    return cleanText\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" clean_my_data() calls clean_my_text for each line of text in a dataset\n",
    "    category  \n",
    "    input: data file containing raw text  \n",
    "    output: data file containing cleaned text entries\n",
    "\"\"\"\n",
    "def clean_my_data(dataList):\n",
    "    print(\"Cleaning all of the data\")\n",
    "    i = 0\n",
    "    for textEntry in dataList:              # reads line of text under \n",
    "                                                    # review category\n",
    "        cleanElement = clean_my_text(textEntry)     # cleans line of text\n",
    "        dataList[i] = cleanElement   # stores cleaned text\n",
    "        i = i + 1\n",
    "        if (i%50 == 0):\n",
    "            print(\"Cleaning review number\", i, \"out of\", len(dataList))\n",
    "    print(\"Finished cleaning all of the data\\n\")\n",
    "    return dataList\n",
    "\n",
    "\n",
    "print(\"Operating on training data...\\n\")\n",
    "reviews = X_train.tolist()\n",
    "cleanReviewData = clean_my_data(reviews)            # cleans the training data\n",
    "\"\"\" create_bag_of_words() generates the bag of words used to evaluate sentiment\n",
    "    input: cleaned dataset\n",
    "    output: tf-idf weighted sparse matrix\n",
    "\"\"\"\n",
    "def create_bag_of_words(X):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "        # use scikit-learn for vectorization\n",
    "    \n",
    "    print ('Generating bag of words...')\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 ngram_range = (1,2), \\\n",
    "                                 max_features = 10000)\n",
    "        # generates vectorization for ngrams of up to 2 words in length\n",
    "        # this will greatly increase feature size, but gives more accurate\n",
    "        # sentiment analysis since some word combinations have large\n",
    "        # impact on sentiment ie: (\"not good\", \"very fast\")\n",
    "                                                         \n",
    "    train_data_features = vectorizer.fit_transform(X)\n",
    "        # vectorizes sparse matrix\n",
    "    train_data_features = train_data_features.toarray()\n",
    "        # convert to a NumPy array for efficient matrix operations\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf_features = tfidf.fit_transform(train_data_features)\n",
    "        # use tf-idf to weight features - places highest sentiment value on\n",
    "        # low-frequency ngrams that are not too uncommon \n",
    "    return vectorizer, tfidf_features, tfidf\n",
    "\n",
    "\n",
    "\n",
    "vectorizer, tfidf_features, tfidf  = (create_bag_of_words(cleanReviewData))   \n",
    "        # stores the sparse matrix of the tf-idf weighted features\n",
    "\n",
    "\n",
    "\"\"\" train_logistic_regression() uses logistic regression model to\n",
    "    evaluate sentiment\n",
    "    options: C sets how strong regularization will be: large C = small amount\n",
    "    input: tf-idf matrix and the sentiment attached to the training example\n",
    "    output: the trained logistic regression model\n",
    "\"\"\"\n",
    "def train_logistic_regression(features, label):\n",
    "    print (\"Training the logistic regression model...\")\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    ml_model = LogisticRegression(C = 100, random_state = 0, solver = 'liblinear')\n",
    "    ml_model.fit(features, label)\n",
    "    print ('Finished training the model\\n')\n",
    "    return ml_model\n",
    "\n",
    "\n",
    "ml_model = train_logistic_regression(tfidf_features, y_train)\n",
    "    # holds the trained model\n",
    "    \n",
    "print(\"Operating on test data...\\n\")\n",
    "sentiments = X_test.tolist()\n",
    "cleanTestData = clean_my_data(sentiments)\n",
    "    # cleans the test data for accuracy evaluation\n",
    "\n",
    "test_data_features = vectorizer.transform(cleanTestData)\n",
    "test_data_features = test_data_features.toarray()\n",
    "    # vectorizes the test data\n",
    "\n",
    "test_data_tfidf_features = tfidf.fit_transform(test_data_features)\n",
    "test_data_tfidf_features = test_data_tfidf_features.toarray()\n",
    "    # tf-idf of test data ngrams\n",
    "\n",
    "predicted_y = ml_model.predict(test_data_tfidf_features)\n",
    "    # uses the trained logistic regression model to assign sentiment to each\n",
    "    # test data example\n",
    "\n",
    "correctly_identified_y = predicted_y == y_test\n",
    "accuracy = np.mean(correctly_identified_y) * 100\n",
    "print ('The accuracy of the model in predicting movie review sentiment is %.0f%%' %accuracy)\n",
    "    # compares the predicted sentiment (predicted_y) vs the actual \n",
    "# value stored in \"sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
